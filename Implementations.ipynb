{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementations.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjmiEvy9kKamp7UmtztRvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/CNNs/blob/master/Implementations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYcHQP1lGXM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81fae54d-c4f2-4f34-fd9d-57ddbf06da95"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__help__=\"you can call VGGnet(kind='vgg16',num_classes=1000,batch_norm=False,pretrained=False) to get a vgg net,\\\n",
        "         you can use __all__ to get the compelete vggnet choose.\\\n",
        "         if you want to use vggxx_bn you should not give the parameter kind='vggxx_bn',\\\n",
        "         you should also give the kind='vggxx_bn' but another parameter batch_norm=True\"\n",
        "\n",
        "__all__=[\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    \n",
        "    def __init__(self,features,num_classes=1000,init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features=features\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Linear(512*7*7,4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096,4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096,num_classes)\n",
        "            )\n",
        "        self.conv1x1=nn.Conv2d(512,num_classes,kernel_size=1,stride=1)\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.features(x)\n",
        "        x=self.conv1x1(x)\n",
        "        x=x.view(x.size(0),-1)\n",
        "        #x=self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias,0)\n",
        "            elif isinstance(m,nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight,1)\n",
        "                nn.init.constant_(m.bias,0)\n",
        "            elif isinstance(m,nn.Linear):\n",
        "                nn.init.normal_(m.weight,0,0.01)\n",
        "                nn.init.constant_(m.bias,0)\n",
        "\n",
        "cfg={\n",
        "    'vgg11':[64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'], #11 weight layers\n",
        "    'vgg13':[64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'], #13 weight layers\n",
        "    'vgg16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'], #16 weight layers\n",
        "    'vgg19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'], #19 weight layers\n",
        "}\n",
        "def make_layers(cfg,batch_norm=False):\n",
        "    layers=[]\n",
        "    in_channels=3\n",
        "    for v in cfg:\n",
        "        if v=='M':\n",
        "            layers+=[nn.MaxPool2d(kernel_size=2,stride=2)]\n",
        "        else:\n",
        "            if batch_norm:\n",
        "                layers+=[nn.Conv2d(in_channels,v,kernel_size=3,padding=1,stride=1,bias=False),\n",
        "                        nn.BatchNorm2d(v),nn.ReLU(True)]\n",
        "            else:\n",
        "                layers+=[nn.Conv2d(in_channels,v,kernel_size=3,padding=1,stride=1),nn.ReLU(True)]\n",
        "            in_channels=v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def VGGnet(kind='vgg16',num_classes=1000,batch_norm=False,pretrained=False,**kwargs):\n",
        "    if pretrained:\n",
        "        kwargs['init_weights']=False\n",
        "        assert num_classes==1000,\\\n",
        "            'pretrained model only on ImageNet which num classes is 1000 but got{}'.format(num_classes)\n",
        "    model=VGG(make_layers(cfg[kind],batch_norm),num_classes,**kwargs)\n",
        "    if pretrained:\n",
        "        name=kind\n",
        "        if batch_norm==True:\n",
        "            name+='_bn'\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls[name]))\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a=nn.Conv2d(1,2,kernel_size=1,bias=False)\n",
        "    print(a.bias)\n",
        "    model=VGGnet(kind='vgg16',num_classes=10,batch_norm=True)\n",
        "    print(model)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            "  (conv1x1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUWPgkPIIsVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f844c97-1ee7-4d7b-8816-6f57921e48d2"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet264', 'densenet29', 'densenet45',\n",
        "           'densenet85']\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features,\n",
        "                                           bn_size * growth_rate, kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, input):\n",
        "        new_features = super(_DenseLayer, self).forward(input)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([input, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "    block_config (list of 4 ints) - how many layers in each pooling block\n",
        "    num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "    bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "      (i.e. bn_size * k features in the bottleneck layer)\n",
        "    drop_rate (float) - dropout rate after each dense layer\n",
        "    num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 16),\n",
        "                 num_init_feature=24, bn_size=4, drop_rate=0, num_classes=1000):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # Firsrt convolution before dense block\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_feature, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_feature)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))])\n",
        "        )\n",
        "\n",
        "        num_features = num_init_feature\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "        self.features.add_module('relu5', nn.ReLU(inplace=True))\n",
        "        self.features.add_module('avgpool', nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "        self.classifier = nn.Conv2d(num_features, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
        "            elif isinstance(m,nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight,1)\n",
        "            elif isinstance(m,nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
        "\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        features = self.features(input)\n",
        "        out = self.classifier(features).view(input.size(0), -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def densenet121(pretrained=False, **kwargs):\n",
        "    model = DenseNet(num_init_feature=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet169(pretrained=False, **kwargs):\n",
        "    model = DenseNet(num_init_feature=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet201(pretrained=False, **kwargs):\n",
        "    model = DenseNet(num_init_feature=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet264(pretrained=False, **kwargs):\n",
        "    model = DenseNet(num_init_feature=64, growth_rate=32, block_config=(6, 12, 64, 48), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "class DenseNet_CIFAR10(nn.Module):\n",
        "    \"\"\"\n",
        "    growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "    block_config (list of 4 ints) - how many layers in each pooling block\n",
        "    num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "    bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "      (i.e. bn_size * k features in the bottleneck layer)\n",
        "    drop_rate (float) - dropout rate after each dense layer\n",
        "    num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 12),\n",
        "                 num_init_feature=24, bn_size=4, drop_rate=0, num_classes=10):\n",
        "        super(DenseNet_CIFAR10, self).__init__()\n",
        "\n",
        "        # Firsrt convolution before dense block\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_feature, kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_feature)),\n",
        "            ('relu0', nn.ReLU(inplace=True))])\n",
        "        )\n",
        "\n",
        "        num_features = num_init_feature\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "        self.features.add_module('relu5', nn.ReLU(inplace=True))\n",
        "        self.features.add_module('avgpool', nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "        self.classifier = nn.Conv2d(num_features, num_classes,kernel_size=1,stride=1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                ps = list(m.parameters())\n",
        "                if len(ps) == 2:\n",
        "                    nn.init.constant_(m.bias,0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        features = self.features(input)\n",
        "        out = self.classifier(features).view(input.size(0), -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def densenet29(pretrained=False, **kwargs):\n",
        "    model = DenseNet_CIFAR10(num_init_feature=48, growth_rate=24, block_config=(6, 6, 6, 6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet45(pretrained=False, **kwargs):\n",
        "    model = DenseNet_CIFAR10(num_init_feature=48, growth_rate=24, block_config=(10, 10, 10, 10), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet85(pretrained=False, **kwargs):\n",
        "    model = DenseNet_CIFAR10(num_init_feature=48, growth_rate=24, block_config=(20, 20, 20, 20), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net = densenet29().to(\"cuda:0\")\n",
        "    import torchsummary\n",
        "\n",
        "    torchsummary.summary(net, input_size=(3, 32, 32))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 48, 32, 32]           1,296\n",
            "       BatchNorm2d-2           [-1, 48, 32, 32]              96\n",
            "              ReLU-3           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 48, 32, 32]              96\n",
            "              ReLU-5           [-1, 48, 32, 32]               0\n",
            "            Conv2d-6           [-1, 96, 32, 32]           4,608\n",
            "       BatchNorm2d-7           [-1, 96, 32, 32]             192\n",
            "              ReLU-8           [-1, 96, 32, 32]               0\n",
            "            Conv2d-9           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-10           [-1, 72, 32, 32]             144\n",
            "             ReLU-11           [-1, 72, 32, 32]               0\n",
            "           Conv2d-12           [-1, 96, 32, 32]           6,912\n",
            "      BatchNorm2d-13           [-1, 96, 32, 32]             192\n",
            "             ReLU-14           [-1, 96, 32, 32]               0\n",
            "           Conv2d-15           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-16           [-1, 96, 32, 32]             192\n",
            "             ReLU-17           [-1, 96, 32, 32]               0\n",
            "           Conv2d-18           [-1, 96, 32, 32]           9,216\n",
            "      BatchNorm2d-19           [-1, 96, 32, 32]             192\n",
            "             ReLU-20           [-1, 96, 32, 32]               0\n",
            "           Conv2d-21           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-22          [-1, 120, 32, 32]             240\n",
            "             ReLU-23          [-1, 120, 32, 32]               0\n",
            "           Conv2d-24           [-1, 96, 32, 32]          11,520\n",
            "      BatchNorm2d-25           [-1, 96, 32, 32]             192\n",
            "             ReLU-26           [-1, 96, 32, 32]               0\n",
            "           Conv2d-27           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-28          [-1, 144, 32, 32]             288\n",
            "             ReLU-29          [-1, 144, 32, 32]               0\n",
            "           Conv2d-30           [-1, 96, 32, 32]          13,824\n",
            "      BatchNorm2d-31           [-1, 96, 32, 32]             192\n",
            "             ReLU-32           [-1, 96, 32, 32]               0\n",
            "           Conv2d-33           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-34          [-1, 168, 32, 32]             336\n",
            "             ReLU-35          [-1, 168, 32, 32]               0\n",
            "           Conv2d-36           [-1, 96, 32, 32]          16,128\n",
            "      BatchNorm2d-37           [-1, 96, 32, 32]             192\n",
            "             ReLU-38           [-1, 96, 32, 32]               0\n",
            "           Conv2d-39           [-1, 24, 32, 32]          20,736\n",
            "      BatchNorm2d-40          [-1, 192, 32, 32]             384\n",
            "             ReLU-41          [-1, 192, 32, 32]               0\n",
            "           Conv2d-42           [-1, 96, 32, 32]          18,432\n",
            "        AvgPool2d-43           [-1, 96, 16, 16]               0\n",
            "      BatchNorm2d-44           [-1, 96, 16, 16]             192\n",
            "             ReLU-45           [-1, 96, 16, 16]               0\n",
            "           Conv2d-46           [-1, 96, 16, 16]           9,216\n",
            "      BatchNorm2d-47           [-1, 96, 16, 16]             192\n",
            "             ReLU-48           [-1, 96, 16, 16]               0\n",
            "           Conv2d-49           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-50          [-1, 120, 16, 16]             240\n",
            "             ReLU-51          [-1, 120, 16, 16]               0\n",
            "           Conv2d-52           [-1, 96, 16, 16]          11,520\n",
            "      BatchNorm2d-53           [-1, 96, 16, 16]             192\n",
            "             ReLU-54           [-1, 96, 16, 16]               0\n",
            "           Conv2d-55           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-56          [-1, 144, 16, 16]             288\n",
            "             ReLU-57          [-1, 144, 16, 16]               0\n",
            "           Conv2d-58           [-1, 96, 16, 16]          13,824\n",
            "      BatchNorm2d-59           [-1, 96, 16, 16]             192\n",
            "             ReLU-60           [-1, 96, 16, 16]               0\n",
            "           Conv2d-61           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-62          [-1, 168, 16, 16]             336\n",
            "             ReLU-63          [-1, 168, 16, 16]               0\n",
            "           Conv2d-64           [-1, 96, 16, 16]          16,128\n",
            "      BatchNorm2d-65           [-1, 96, 16, 16]             192\n",
            "             ReLU-66           [-1, 96, 16, 16]               0\n",
            "           Conv2d-67           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-68          [-1, 192, 16, 16]             384\n",
            "             ReLU-69          [-1, 192, 16, 16]               0\n",
            "           Conv2d-70           [-1, 96, 16, 16]          18,432\n",
            "      BatchNorm2d-71           [-1, 96, 16, 16]             192\n",
            "             ReLU-72           [-1, 96, 16, 16]               0\n",
            "           Conv2d-73           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-74          [-1, 216, 16, 16]             432\n",
            "             ReLU-75          [-1, 216, 16, 16]               0\n",
            "           Conv2d-76           [-1, 96, 16, 16]          20,736\n",
            "      BatchNorm2d-77           [-1, 96, 16, 16]             192\n",
            "             ReLU-78           [-1, 96, 16, 16]               0\n",
            "           Conv2d-79           [-1, 24, 16, 16]          20,736\n",
            "      BatchNorm2d-80          [-1, 240, 16, 16]             480\n",
            "             ReLU-81          [-1, 240, 16, 16]               0\n",
            "           Conv2d-82          [-1, 120, 16, 16]          28,800\n",
            "        AvgPool2d-83            [-1, 120, 8, 8]               0\n",
            "      BatchNorm2d-84            [-1, 120, 8, 8]             240\n",
            "             ReLU-85            [-1, 120, 8, 8]               0\n",
            "           Conv2d-86             [-1, 96, 8, 8]          11,520\n",
            "      BatchNorm2d-87             [-1, 96, 8, 8]             192\n",
            "             ReLU-88             [-1, 96, 8, 8]               0\n",
            "           Conv2d-89             [-1, 24, 8, 8]          20,736\n",
            "      BatchNorm2d-90            [-1, 144, 8, 8]             288\n",
            "             ReLU-91            [-1, 144, 8, 8]               0\n",
            "           Conv2d-92             [-1, 96, 8, 8]          13,824\n",
            "      BatchNorm2d-93             [-1, 96, 8, 8]             192\n",
            "             ReLU-94             [-1, 96, 8, 8]               0\n",
            "           Conv2d-95             [-1, 24, 8, 8]          20,736\n",
            "      BatchNorm2d-96            [-1, 168, 8, 8]             336\n",
            "             ReLU-97            [-1, 168, 8, 8]               0\n",
            "           Conv2d-98             [-1, 96, 8, 8]          16,128\n",
            "      BatchNorm2d-99             [-1, 96, 8, 8]             192\n",
            "            ReLU-100             [-1, 96, 8, 8]               0\n",
            "          Conv2d-101             [-1, 24, 8, 8]          20,736\n",
            "     BatchNorm2d-102            [-1, 192, 8, 8]             384\n",
            "            ReLU-103            [-1, 192, 8, 8]               0\n",
            "          Conv2d-104             [-1, 96, 8, 8]          18,432\n",
            "     BatchNorm2d-105             [-1, 96, 8, 8]             192\n",
            "            ReLU-106             [-1, 96, 8, 8]               0\n",
            "          Conv2d-107             [-1, 24, 8, 8]          20,736\n",
            "     BatchNorm2d-108            [-1, 216, 8, 8]             432\n",
            "            ReLU-109            [-1, 216, 8, 8]               0\n",
            "          Conv2d-110             [-1, 96, 8, 8]          20,736\n",
            "     BatchNorm2d-111             [-1, 96, 8, 8]             192\n",
            "            ReLU-112             [-1, 96, 8, 8]               0\n",
            "          Conv2d-113             [-1, 24, 8, 8]          20,736\n",
            "     BatchNorm2d-114            [-1, 240, 8, 8]             480\n",
            "            ReLU-115            [-1, 240, 8, 8]               0\n",
            "          Conv2d-116             [-1, 96, 8, 8]          23,040\n",
            "     BatchNorm2d-117             [-1, 96, 8, 8]             192\n",
            "            ReLU-118             [-1, 96, 8, 8]               0\n",
            "          Conv2d-119             [-1, 24, 8, 8]          20,736\n",
            "     BatchNorm2d-120            [-1, 264, 8, 8]             528\n",
            "            ReLU-121            [-1, 264, 8, 8]               0\n",
            "          Conv2d-122            [-1, 132, 8, 8]          34,848\n",
            "       AvgPool2d-123            [-1, 132, 4, 4]               0\n",
            "     BatchNorm2d-124            [-1, 132, 4, 4]             264\n",
            "            ReLU-125            [-1, 132, 4, 4]               0\n",
            "          Conv2d-126             [-1, 96, 4, 4]          12,672\n",
            "     BatchNorm2d-127             [-1, 96, 4, 4]             192\n",
            "            ReLU-128             [-1, 96, 4, 4]               0\n",
            "          Conv2d-129             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-130            [-1, 156, 4, 4]             312\n",
            "            ReLU-131            [-1, 156, 4, 4]               0\n",
            "          Conv2d-132             [-1, 96, 4, 4]          14,976\n",
            "     BatchNorm2d-133             [-1, 96, 4, 4]             192\n",
            "            ReLU-134             [-1, 96, 4, 4]               0\n",
            "          Conv2d-135             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-136            [-1, 180, 4, 4]             360\n",
            "            ReLU-137            [-1, 180, 4, 4]               0\n",
            "          Conv2d-138             [-1, 96, 4, 4]          17,280\n",
            "     BatchNorm2d-139             [-1, 96, 4, 4]             192\n",
            "            ReLU-140             [-1, 96, 4, 4]               0\n",
            "          Conv2d-141             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-142            [-1, 204, 4, 4]             408\n",
            "            ReLU-143            [-1, 204, 4, 4]               0\n",
            "          Conv2d-144             [-1, 96, 4, 4]          19,584\n",
            "     BatchNorm2d-145             [-1, 96, 4, 4]             192\n",
            "            ReLU-146             [-1, 96, 4, 4]               0\n",
            "          Conv2d-147             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-148            [-1, 228, 4, 4]             456\n",
            "            ReLU-149            [-1, 228, 4, 4]               0\n",
            "          Conv2d-150             [-1, 96, 4, 4]          21,888\n",
            "     BatchNorm2d-151             [-1, 96, 4, 4]             192\n",
            "            ReLU-152             [-1, 96, 4, 4]               0\n",
            "          Conv2d-153             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-154            [-1, 252, 4, 4]             504\n",
            "            ReLU-155            [-1, 252, 4, 4]               0\n",
            "          Conv2d-156             [-1, 96, 4, 4]          24,192\n",
            "     BatchNorm2d-157             [-1, 96, 4, 4]             192\n",
            "            ReLU-158             [-1, 96, 4, 4]               0\n",
            "          Conv2d-159             [-1, 24, 4, 4]          20,736\n",
            "     BatchNorm2d-160            [-1, 276, 4, 4]             552\n",
            "            ReLU-161            [-1, 276, 4, 4]               0\n",
            "AdaptiveAvgPool2d-162            [-1, 276, 1, 1]               0\n",
            "          Conv2d-163             [-1, 10, 1, 1]           2,770\n",
            "================================================================\n",
            "Total params: 964,426\n",
            "Trainable params: 964,426\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 41.24\n",
            "Params size (MB): 3.68\n",
            "Estimated Total Size (MB): 44.93\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiC6Ypf3ZPZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9413200-e3d5-4f48-f9e4-f147f51e3a5f"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self,channel,batch_norm=False):\n",
        "        super(Inception, self).__init__()\n",
        "        if batch_norm==False:\n",
        "            self.branch1x1=nn.Conv2d(channel[0],channel[1],kernel_size=(1,1),stride=1)\n",
        "\n",
        "            self.branch3x3_1=nn.Conv2d(channel[0],channel[2],kernel_size=(1,1),stride=1)\n",
        "            self.branch3x3_2=nn.Conv2d(channel[2],channel[3],kernel_size=(3,3),stride=1,padding=1)\n",
        "\n",
        "            self.branch5x5_1=nn.Conv2d(channel[0],channel[4],kernel_size=(1,1),stride=1)\n",
        "            self.branch5x5_2=nn.Conv2d(channel[4],channel[5],kernel_size=(5,5),stride=1,padding=2)\n",
        "\n",
        "            self.branchM_1=nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
        "            self.branchM_2=nn.Conv2d(channel[0],channel[6],kernel_size=(1,1),stride=1)\n",
        "        else:\n",
        "            self.branch1x1=BasicConv2d(channel[0],channel[1],kernel_size=(1,1),stride=1)\n",
        "\n",
        "            self.branch3x3_1=BasicConv2d(channel[0],channel[2],kernel_size=(1,1),stride=1)\n",
        "            self.branch3x3_2=BasicConv2d(channel[2],channel[3],kernel_size=(3,3),stride=1,padding=1)\n",
        "\n",
        "            self.branch5x5_1=BasicConv2d(channel[0],channel[4],kernel_size=(1,1),stride=1)\n",
        "            self.branch5x5_2=BasicConv2d(channel[4],channel[5],kernel_size=(5,5),stride=1,padding=2)\n",
        "\n",
        "            self.branchM_1=nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
        "            self.branchM_2=BasicConv2d(channel[0],channel[6],kernel_size=(1,1),stride=1)\n",
        "\n",
        "        self.relu=nn.ReLU(True)\n",
        "\n",
        "    def forward(self,x):\n",
        "        branch1x1=self.relu(self.branch1x1(x))\n",
        "\n",
        "        branch3x3_1=self.relu(self.branch3x3_1(x))\n",
        "        branch3x3_2=self.relu(self.branch3x3_2(branch3x3_1))\n",
        "\n",
        "        branch5x5_1=self.relu(self.branch5x5_1(x))\n",
        "        branch5x5_2=self.relu(self.branch5x5_2(branch5x5_1))\n",
        "\n",
        "        branchM_1=self.relu(self.branchM_1(x))\n",
        "        branchM_2=self.relu(self.branchM_2(branchM_1))\n",
        "\n",
        "        outputs = [branch1x1, branch3x3_2, branch5x5_2, branchM_2]\n",
        "\n",
        "        return torch.cat(outputs,1)\n",
        "\n",
        "\n",
        "channel=[\n",
        "    [192, 64, 96,128, 16, 32, 32],#3a\n",
        "    [256,128,128,192, 32, 96, 64],#3b\n",
        "    [480,192, 96,208, 16, 48, 64],#4a\n",
        "    [512,160,112,224, 24, 64, 64],#4b\n",
        "    [512,128,128,256, 24, 64, 64],#4c\n",
        "    [512,112,144,288, 32, 64, 64],#4d\n",
        "    [528,256,160,320, 32,128,128],#4e\n",
        "    [832,256,160,320, 32,128,128],#5a\n",
        "    [832,384,192,384, 48,128,128] #5b\n",
        "]\n",
        "class InceptionNet(nn.Module):\n",
        "    def __init__(self,num_classes=1000,batch_norm=False):\n",
        "        super(InceptionNet, self).__init__()\n",
        "        \n",
        "        if num_classes==10:\n",
        "            channel[0][0]=64\n",
        "            self.begin=nn.Sequential(\n",
        "                nn.Conv2d(3,64,kernel_size=3,stride=1),\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(64,64,kernel_size=3,stride=1),\n",
        "                nn.ReLU(True)\n",
        "            )\n",
        "\n",
        "            self.auxout1=nn.Sequential(\n",
        "                nn.Conv2d(512,512,kernel_size=5,stride=3), #4x4x512\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(512,128,kernel_size=1),          #4x4x128\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(128, 10,kernel_size=4)           #1x1x10\n",
        "            )\n",
        "            self.auxout2=nn.Sequential(\n",
        "                nn.Conv2d(528,528,kernel_size=5,stride=3), #4x4x528,\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(528,128,kernel_size=1),          #4x4x128,\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(128, 10,kernel_size=4)           #1x1x10\n",
        "            )\n",
        "        else:\n",
        "            self.begin=nn.Sequential(\n",
        "                nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3),\n",
        "                nn.ReLU(True),\n",
        "                nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
        "                nn.Conv2d(64,192,kernel_size=3,stride=1,padding=1),\n",
        "                nn.ReLU(True),\n",
        "                nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
        "            )\n",
        "            self.auxout1=nn.Sequential(\n",
        "                nn.Conv2d(512,512,kernel_size=5,stride=3),#4x4x512\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(512,128,kernel_size=1),        #4x4x128 \n",
        "                nn.ReLU(True)  \n",
        "            )\n",
        "            self.auxout12=nn.Sequential(\n",
        "                nn.Linear(2048,1024),           \n",
        "                nn.Dropout(0.5),\n",
        "                nn.linear(1024,num_classes)  \n",
        "            )\n",
        "                \n",
        "            self.auxout2=nn.Sequential(\n",
        "                nn.Conv2d(528,528,kernel_size=5,stride=3),#4x4x528\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(528,128,kernel_size=1),         #4x4x128   \n",
        "                nn.ReLU(True)\n",
        "            )\n",
        "            self.auxout22=nn.Sequential(\n",
        "                nn.Linear(2048,1024),           \n",
        "                nn.Dropout(0.5),\n",
        "                nn.linear(1024,num_classes)  \n",
        "            )\n",
        "\n",
        "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        self.inception3a=Inception(channel[0],batch_norm)\n",
        "        self.inception3b=Inception(channel[1],batch_norm)\n",
        "\n",
        "        self.inception4a=Inception(channel[2],batch_norm)\n",
        "        self.inception4b=Inception(channel[3],batch_norm)\n",
        "        self.inception4c=Inception(channel[4],batch_norm)\n",
        "        self.inception4d=Inception(channel[5],batch_norm)\n",
        "        self.inception4e=Inception(channel[6],batch_norm)\n",
        "        \n",
        "        self.inception5a=Inception(channel[7],batch_norm)\n",
        "        self.inception5b=Inception(channel[8],batch_norm)\n",
        "\n",
        "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "        \n",
        "        self.conv1x1=nn.Conv2d(1024,num_classes,kernel_size=1)\n",
        "        \n",
        "        self._initialize_weights()\n",
        "\n",
        "        '''\n",
        "        #follow the original papar,but for the computation ,I do not use it\n",
        "        self.drop=nn.Dropout()\n",
        "        self.linear=nn.Linear(1024,1000)\n",
        "        '''\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias,0)\n",
        "            elif isinstance(m,nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight,1)\n",
        "                nn.init.constant_(m.bias,0)\n",
        "            elif isinstance(m,nn.Linear):\n",
        "                nn.init.normal_(m.weight,0,0.01)\n",
        "                nn.init.constant_(m.bias,0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.begin(x)\n",
        "\n",
        "        x=self.inception3a(x)\n",
        "        x=self.inception3b(x)\n",
        "        x=self.maxpool(x)\n",
        "\n",
        "        x=self.inception4a(x)\n",
        "        auxout1=self.auxout1(x)\n",
        "        auxout1=auxout1.view(auxout1.size(0),-1)\n",
        "        #if you use this network to train on ImageNet you should add this code\n",
        "        #auxout1=self.auxout12(auxout1)\n",
        "        x=self.inception4b(x)\n",
        "        x=self.inception4c(x)\n",
        "        x=self.inception4d(x)\n",
        "\n",
        "        auxout2=self.auxout2(x)\n",
        "        auxout2=auxout2.view(auxout2.size(0),-1)\n",
        "        #if you use this network to train on ImageNet you should add this code\n",
        "        #auxout2=self.auxout22(auxout2)\n",
        "        x=self.inception4e(x)\n",
        "        x=self.maxpool(x)\n",
        "\n",
        "        x=self.inception5a(x)\n",
        "        x=self.inception5b(x)\n",
        "        x=self.avgpool(x)\n",
        "\n",
        "        outputs=self.conv1x1(x)\n",
        "        outputs=outputs.view(outputs.size(0),-1)\n",
        "\n",
        "        return outputs,auxout1,auxout2\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net=InceptionNet(num_classes=10,batch_norm=True)\n",
        "    print(net)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InceptionNet(\n",
            "  (begin): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (auxout1): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 10, kernel_size=(4, 4), stride=(1, 1))\n",
            "  )\n",
            "  (auxout2): Sequential(\n",
            "    (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 10, kernel_size=(4, 4), stride=(1, 1))\n",
            "  )\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (inception3a): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception3b): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception4a): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception4b): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception4c): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception4d): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception4e): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception5a): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (inception5b): Inception(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branchM_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (branchM_2): BasicConv2d(\n",
            "      (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (conv1x1): Conv2d(1024, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbthX7YmdL5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version=1.0, num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        if version not in [1.0, 1.1]:\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1.0 or 1.1 expected\".format(version=version))\n",
        "        self.num_classes = num_classes\n",
        "        if version == 1.0:\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        # Final convolution is initialized differently form the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x.view(x.size(0), self.num_classes)\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = SqueezeNet(version=1.0, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_0']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = SqueezeNet(version=1.1, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_1']))\n",
        "    return model\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        net=SqueezeNet(version=1.1, num_classes=100)\n",
        "    print(net)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}